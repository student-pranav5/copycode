
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Copy Text Buttons</title>
    <style>
        body {
            background-color: #f0f0f0;
            font-family: Arial, sans-serif;
            text-align: center;
        }

        button {
            margin: 10px;
            padding: 10px;
            font-size: 16px;
            background-color: #ddd;
            border: 1px solid #aaa;
            cursor: pointer;
        }

        button:hover {
            background-color: #ccc;
        }

        #copiedMsg {
            margin-top: 20px;
            font-weight: bold;
            color: green;
        }
    </style>
</head>

<body>

    <h1>Copy Text Example</h1>

    <!-- Buttons for each code block -->
    <button onclick="copyText(text1)">csv2h</button>
    <button onclick="copyText(text2)">x2h</button>
    <button onclick="copyText(text3)">j2h</button>
    <button onclick="copyText(text4)">sql2h</button>
    <button onclick="copyText(text5)">pic2h</button>
    <button onclick="copyText(text6)">v2h</button>
    <button onclick="copyText(text7)">a2h</button>
    <button onclick="copyText(text8)">fixer</button>
    <button onclick="copyText(text9)">binbuck</button>
    <button onclick="copyText(text10)">aggreg</button>
    <button onclick="copyText(text11)">outlier</button>
    <button onclick="copyText(text12)">audit</button>
    <button onclick="copyText(text13)">dataprocessR</button>
    <button onclick="copyText(text14)">retrieveAttributes</button>
    <button onclick="copyText(text15)">DataPattern</button>
    <button onclick="copyText(text16)">LoadingIP_DATA</button>
    <button onclick="copyText(text17)">errorManage</button>
    

    <p id="copiedMsg"></p>

    <script>
        // Text blocks (only a few shown here for demonstration; add all your blocks similarly)
        const text1 = `
import pandas as pd
sInputFileName='C:/Users/NANDINI/OneDrive/Documents/Power BI Desktop/Pizza_sales/pizzas.csv'
InputData=pd.read_csv(sInputFileName,encoding="latin-1")
print('Input Data Values ===================================')
print(InputData)
print('=====================================================')
ProcessData=InputData
# Remove columns ISO-2-Code and ISO-3-CODE
ProcessData.drop('size', axis=1,inplace=True)
ProcessData.drop('pizza_type_id', axis=1,inplace=True)
# Rename Country and ISO-M49
ProcessData.rename(columns={'pizza_id': 'pid'}, inplace=True)
ProcessData.rename(columns={'price': 'Amount'}, inplace=True)
# Set new Index
ProcessData.set_index('pid', inplace=True)
# Sort data by CurrencyNumber
ProcessData.sort_values('Amount', axis=0, ascending=False, inplace=True)
print('Process Data Values =================================')
print(ProcessData)
print('=====================================================')
OutputData=ProcessData
sOutputFileName = 'C:/Users/NANDINI/OneDrive/Documents/Power BI Desktop/Pizza_sales/pizzaaa.csv'
OutputData.to_csv(sOutputFileName, index = False)
print('CSV to HORUS - Done')
        `;
        const text2 = `
import pandas as pd
import xml.etree.ElementTree as ET

def df2xml(data):
 header = data.columns
 root = ET.Element('root')
 for row in range(data.shape[0]):
 entry = ET.SubElement(root,'entry')
 for index in range(data.shape[1]):
 schild=str(header[index])
 child = ET.SubElement(entry, schild)
 if str(data[schild][row]) != 'nan':
 child.text = str(data[schild][row])
 else:
 child.text = 'n/a'
 entry.append(child)
 result = ET.tostring(root)
 return result

def xml2df(xml_data):
 root = ET.XML(xml_data)
 all_records = []
 for i, child in enumerate(root):
 record = {}
 for subchild in child:
 record[subchild.tag] = subchild.text
 all_records.append(record)
 return pd.DataFrame(all_records)

sInputFileName='D:\MSCIT\ds prac\Country_Code.xml'
InputData = open(sInputFileName).read()
print('=====================================================')
print('Input Data Values ===================================')
print('=====================================================')
print(InputData)
print('=====================================================')

ProcessDataXML=InputData
# XML to Data Frame
ProcessData=xml2df(ProcessDataXML)
# Remove columns ISO-2-Code and ISO-3-CODE
ProcessData.drop('ISO-2-CODE', axis=1,inplace=True)
ProcessData.drop('ISO-3-Code', axis=1,inplace=True)
# Rename Country and ISO-M49
ProcessData.rename(columns={'Country': 'CountryName'}, inplace=True)
ProcessData.rename(columns={'ISO-M49': 'CountryNumber'}, inplace=True)
# Set new Index
ProcessData.set_index('CountryNumber', inplace=True)
# Sort data by CurrencyNumber
ProcessData.sort_values('CountryName', axis=0, ascending=False, inplace=True)
print('=====================================================')
print('Process Data Values =================================')
print('=====================================================')
print(ProcessData)
print('=====================================================')
#=============================================================
# Output Agreement ===========================================
#=============================================================
OutputData=ProcessData
sOutputFileName = 'D:\MSCIT\ds prac\HORUS-XML-Country.csv'
OutputData.to_csv(sOutputFileName, index = False)
print('=====================================================')
print('XML to HORUS - Done')
print('=====================================================')
# Utility done ===============================================

`;
        const text3 = `
# Utility Start JSON to HORUS ================================= 
# Standard Tools 
import pandas as pd 
# Input Agreement ============================================ 
sInputFileName='D:\MSCIT\ds prac\Country_Code.json' 
InputData=pd.read_json(sInputFileName, orient='index', encoding="latin-1") 
print('Input Data Values ===================================') 
print(InputData) 
print('=====================================================') 
# Processing Rules =========================================== 
ProcessData=InputData # Remove columns ISO-2-Code and ISO-3-CODE 
ProcessData.drop('ISO-2-CODE', axis=1,inplace=True) 
ProcessData.drop('ISO-3-Code', axis=1,inplace=True) 
# Rename Country and ISO-M49 
ProcessData.rename(columns={'Country': 'CountryName'}, inplace=True) 
ProcessData.rename(columns={'ISO-M49': 'CountryNumber'}, inplace=True) 
# Set new Index ProcessData.set_index('CountryNumber', inplace=True) 
# Sort data by CurrencyNumber 
ProcessData.sort_values('CountryName',axis=0, ascending=False, inplace=True) 
print('Process Data Values =================================') 
print(ProcessData) print('=====================================================') 
# Output Agreement =========================================== 
OutputData=ProcessData sOutputFileName='D:\MSCIT\ds prac\HORUS-JSON-Country.csv' 
OutputData.to_csv(sOutputFileName, index = False) print('JSON to HORUS - Done') 
# Utility done ===============================================

        `;
        const text4 = `
import sqlite3 as sq 
# Input Agreement ============================================ 
sInputFileName='D:/MSCIT/ds prac/utility.db' 
sInputTable='Country_Code' conn = sq.connect(sInputFileName) 
sSQL='select * FROM ' + sInputTable + ';' 
InputData=pd.read_sql_query(sSQL, conn) 
print('Input Data Values ===================================') 
print(InputData) print('=====================================================') # Processing Rules =========================================== 
ProcessData=InputData 
# Remove columns ISO-2-Code and ISO-3-CODE 
ProcessData.drop('ISO-2-CODE', axis=1,inplace=True) 
ProcessData.drop('ISO-3-Code', axis=1,inplace=True) 
# Rename Country and ISO-M49 
ProcessData.rename(columns={'Country': 'CountryName'}, inplace=True) ProcessData.rename(columns={'ISO-M49': 'CountryNumber'}, inplace=True) 
# Set new Index 
ProcessData.set_index('CountryNumber', inplace=True) 
# Sort data by CurrencyNumber 
ProcessData.sort_values('CountryName', axis=0, ascending=False, inplace=True) print('Process Data Values =================================') print(ProcessData) print('=====================================================') # Output Agreement =========================================== OutputData=ProcessData sOutputFileName='D:\MSCIT\ds prac\HORUS-CSV-Country.csv' OutputData.to_csv(sOutputFileName, index = False) 
print('Database to HORUS - Done') 
        `;
        const text5 = `
(Spyder)
# Utility Start Picture to HORUS ================================= 
# Standard Tools 
import imageio.v2 as imageio 
# Explicitly use imageio.v2 to avoid deprecation warnings 
import pandas as pd 
import matplotlib.pyplot as plt 
import numpy as np 
# Input Agreement ============================================ sInputFileName = r'D:\MSCIT\ds prac\Angus.jpg' # Using a raw string for the path InputData = imageio.imread(sInputFileName) 
print('Input Data Values ===================================') 
print('X: ', InputData.shape[0]) 
print('Y: ', InputData.shape[1]) 
print('Channels (e.g., RGB/RGBA): ', InputData.shape[2]) print('=====================================================') 
# Processing Rules =========================================== ProcessRawData = InputData.flatten() 
channels = InputData.shape[2] # Number of color channels (3 for RGB, 4 for RGBA) y = channels + 2 
x = int(ProcessRawData.shape[0] / y) 
ProcessData = pd.DataFrame(np.reshape(ProcessRawData, (x, y))) 
# Define columns based on the number of channels 
if channels == 3: 
sColumns = ['XAxis', 'YAxis', 'Red', 'Green', 'Blue'] 
elif channels == 4: 
sColumns = ['XAxis', 'YAxis', 'Red', 'Green', 'Blue', 'Alpha'] 
else: 
raise ValueError("Unexpected number of channels in the image.") ProcessData.columns = sColumns 
ProcessData.index.names = ['ID'] 
print('Rows: ', ProcessData.shape[0]) 
print('Columns :', ProcessData.shape[1]) print('=====================================================') print('Process Data Values =================================') print('=====================================================') 
# Display the image 
plt.imshow(InputData) 
plt.show() print('=====================================================') 
# Output Agreement =========================================== OutputData = ProcessData print('Storing File') 
sOutputFileName = r'D:\MSCIT\ds prac\HORUS-Picture.csv' 
# Using a raw string for the path OutputData.to_csv(sOutputFileName, index=False) print('=====================================================') print('Picture to HORUS - Done') print('=====================================================') 
# Utility done ===============================================

        `;

        const text6 = `
        # Utility Start Movie to HORUS (Part 1) ====================== 
# Standard Tools 
import os 
import shutil 
import cv2 
# Input and Output Directories 
sInputFileName = r'D:\MSCIT\ds prac\Dog.mp4' 
# Use raw string for Windows path sDataBaseDir = r'D:\MSCIT\ds prac\temp' 
# Use raw string for Windows path 
# Clear and recreate the output directory 
if os.path.exists(sDataBaseDir): 
shutil.rmtree(sDataBaseDir) 
os.makedirs(sDataBaseDir, exist_ok=True) print('=====================================================') print('Start Movie to Frames') print('=====================================================') 
# Open the video file 
vidcap = cv2.VideoCapture(sInputFileName) 
success, image = vidcap.read() 
count = 0 
while success and count <= 100: # Stop after 100 frames 
# Frame file name 
sFrame = os.path.join(sDataBaseDir, f'dog-frame-{count:04d}.jpg') 
# Save frame as image 
if success: 
cv2.imwrite(sFrame, image) 
print('Extracted:', sFrame) 
# Remove empty frames if any (file size check) 
if os.path.getsize(sFrame) == 0: 
os.remove(sFrame) 
print('Removed empty frame:', sFrame) 
# Read next frame success, image = vidcap.read() count += 1 print('=====================================================') print('Generated:', count, 'Frames') print('=====================================================') print('Movie to Frames HORUS - Done') print('=====================================================')
        `

        const text7 = `
        # Utility Start Audio to HORUS =============================== 
# Standard Tools  
from scipy.io import wavfile 
import pandas as pd 
import matplotlib.pyplot as plt 
import numpy as np  
def show_info(aname, a, r): 
print('----------------') 
print("Audio:", aname) 
print('----------------') 
print("Rate:", r) 
print('----------------') 
print("Shape:", a.shape) 
print("Dtype:", a.dtype) 
print("Min, Max:", a.min(), a.max()) print('----------------') 
plot_info(aname, a, r) #============================================================def plot_info(aname, a, r): 
sTitle = 'Signal Wave - ' + aname + ' at ' + str(r) + ' Hz' 
plt.title(sTitle) 
sLegend = [] 
for c in range(a.shape[1]): 
sLabel = 'Ch' + str(c + 1) 
sLegend.append(sLabel) 
plt.plot(a[:, c], label=sLabel) 
plt.legend(sLegend) 
plt.show()
#=======================================================
# Process Audio File Function 
def process_audio_file(file_path, output_csv, channel_count):         print('=====================================================') print('Processing : ', file_path) print('=====================================================') InputRate, InputData = wavfile.read(file_path) 
show_info(f"{channel_count} channel", InputData, InputRate) 
ProcessData = pd.DataFrame(InputData) 
sColumns = [f'Ch{i+1}' for i in range(channel_count)] 
ProcessData.columns = sColumns 
try: 
ProcessData.to_csv(output_csv, index=False) 
print(f'Saved CSV file: {output_csv}') 
except PermissionError: 
print(f"Permission denied for file: {output_csv}. Please close the file if open and try again.")
#======================================================= 
# Process each audio file 
process_audio_file(r'D:\MSCIT\ds prac\2ch-sound.wav', r'D:\MSCIT\ds prac\HORUSAudio-2ch.csv', 2) 
process_audio_file(r'D:\MSCIT\ds prac\4ch-sound.wav', r'D:\MSCIT\ds prac\HORUSAudio-4ch.csv', 4) 
process_audio_file(r'D:\MSCIT\ds prac\6ch-sound.wav', r'D:\MSCIT\ds prac\HORUSAudio-6ch.csv', 6) 
process_audio_file(r'D:\MSCIT\ds prac\8Ch-sound.wav', r'D:\MSCIT\ds prac\HORUSAudio-8ch.csv', 8) print('=====================================================') print('Audio to HORUS - Done') print('=====================================================') #============================================================

        `
        const text8 = `
        # Program to Demonstrate Fixers utilities
import string
import datetime as dt

# 1. Removing leading and lagging spaces from data entry
baddata = "  Data Science with too many spaces is bad!!!"
print('>', baddata, '<')  # Original data with spaces
cleandata = baddata.strip()
print('>', cleandata, '<')  # Cleaned data without spaces

# 2. Removing nonprintable characters from data entry
printable = set(string.printable)
baddata = "Data\x00science with\x02 funny characters is \x10bad!!!"
cleandata = "".join(filter(lambda x: x in printable, baddata))
print('Bad Data: ', baddata)
print('Clean Data: ', cleandata)

# 3. Reformatting data entry to match specific formatting criteria (convert YYYY-MM-DD to DD Month YYYY)
baddate = dt.date(2019, 10, 31)
baddata = format(baddate, '%Y-%m-%d')  # Original date format
gooddate = dt.datetime.strptime(baddata, '%Y-%m-%d')  # Parse to datetime object
gooddata = format(gooddate, '%d-%B-%Y')  # Reformat to desired format
print('Bad Data: ', baddata)
print('Clean Data: ', gooddata

        `
        const text9 = `
        # Data Binning and Bucketing
import numpy as np
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
import scipy.stats as stats
np.random.seed(0)
#example data
mu=90#mean of distribution
sigma=25#sd of deviation
x=mu+sigma*np.random.randn(5000)
num_bins=25
fig,ax=plt.subplots()

#the histogram of the data
n, bins, patches = ax.hist(x, num_bins, density=1)

#add a best-fit line
y=stats.norm.pdf(bins,mu,sigma)
ax.plot(bins,y,'--')
ax.set_xlabel('Example Data')
ax.set_ylabel('Probability Density')
sTitle = r'Histogram '+str(len(x)) + 'entries intro' + str(num_bins) + 'Bins:$\mu=' +str(mu) +'$,$\sigma=' + str(sigma) + '$'
ax.set_title(sTitle)
fig.tight_layout()
sPathFig='C:/Users/NANDINI/OneDrive/Documents/Roshni/Java/Histogram.png'
fig.savefig(sPathFig)
plt.show()

        `
        const text10 = `
        # Averaging of Data
import pandas as pd

# Input and Output file paths
InputFileName = 'C:/Users/NANDINI/OneDrive/Documents/Roshni/Projects/Python_Diwali_Sales_Analysis/Diwali Sales Data.csv'
OutputFileName = 'C:/Users/NANDINI/OneDrive/Documents/Roshni/Projects/Python_Diwali_Sales_Analysis/Diwali_Sales_Data_Avg.csv'

Base = 'C:/Users/NANDINI'
print('#################')
print('Working Base: ', Base, 'using')
print('#################')

# Use the InputFileName directly since it already contains the full path
sFileName = InputFileName
print('Loading: ', sFileName)

# Read the CSV file with specified columns
IP_DATA_ALL = pd.read_csv(
    sFileName,
    header=0,
    low_memory=False,
    usecols=['User_ID', 'Cust_name', 'Product_ID', 'Gender'],
    encoding="latin-1"
)

# Rename columns for consistency
IP_DATA_ALL.rename(columns={'Customer Name': 'Cust_name'}, inplace=True)

# Select specific columns
AllData = IP_DATA_ALL[['User_ID', 'Cust_name', 'Gender']]

# Print all data
print(AllData)

# Group by 'Country' and 'Place_Name' and calculate the mean of 'Latitude'
MeanData = AllData.groupby(['User_ID', 'Cust_name'])['Product_ID'].mean()

# Print the grouped mean data
print(MeanData)

# Optionally save the output to a new file
MeanData.to_csv(OutputFileName, header=True)
print('Processed data saved to:', OutputFileName)
        `
        const text11 = `
        import pandas as pd

# Input and Output filenames
InputFileName = "Diwali Sales Data.csv"
OutputFileName = "Retrieve_Outliers.csv"
Base = "C:/Users/NANDINI/OneDrive/Documents/Roshni/Projects/Python_Diwali_Sales_Analysis"

print("############################################")
print("Working Base :", Base)
print("############################################")

# Load the input file with the correct path
file_path = f"{Base}/{InputFileName}"
print("Loading :", file_path)
data = pd.read_csv(file_path, encoding='latin-1')

# Assuming we are focusing on a specific column for outliers (e.g., 'Amount')
column_to_analyze = 'Amount'

# Grouping data (if required)
MeanData = data[column_to_analyze].mean()
StdData = data[column_to_analyze].std()

print("Outliers")
UpperBound = MeanData + StdData
LowerBound = MeanData - StdData

print("Higher than ", UpperBound)
OutliersHigher = data[data[column_to_analyze] > UpperBound]
print(OutliersHigher)

print("Lower than ", LowerBound)
OutliersLower = data[data[column_to_analyze] < LowerBound]
print(OutliersLower)

# Combine and export outliers
Outliers = pd.concat([OutliersHigher, OutliersLower])
Outliers.to_csv(f"{Base}/{OutputFileName}", index=False)
        `
        const text12 = `
        import sys
import os
import logging
import uuid
import shutil
import time

###############################################################
Base="C:/Users/NANDINI/OneDrive/Documents/Roshni/Projects"

###############################################################
sCompanies=['01-Vermeulen','02-Krennwallner','03-Hillman','04-Clark']
sLayers=['01-Retrieve','02-Assess','03-Process','04-Transform','05-Organise','06-Report']
sLevels=['debug','info','warning','error']

for sCompany in sCompanies:
    sFileDir=Base + '/' + sCompany
    if not os.path.exists(sFileDir):
        os.makedirs(sFileDir)
    for sLayer in sLayers:
        log = logging.getLogger() # root logger
        for hdlr in log.handlers[:]: # remove all old handlers
            log.removeHandler(hdlr)
        #######################################################
        sFileDir=Base + '/' + sCompany + '/' + sLayer + '/Logging'
        if os.path.exists(sFileDir):
            shutil.rmtree(sFileDir)
        time.sleep(2)
        os.makedirs(sFileDir)

        skey=str(uuid.uuid4())
        sLogFile=Base + '/' + sCompany + '/' + sLayer + '/Logging/Logging_'+skey+'.log'
        print('Set up:', sLogFile)
        
        # set up logging to file
        logging.basicConfig(level=logging.DEBUG,
                            format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',
                            datefmt='%m-%d %H:%M',
                            filename=sLogFile,
                            filemode='w')

        # define a Handler which writes INFO messages or higher to the sys.stderr
        console = logging.StreamHandler()
        console.setLevel(logging.INFO)
        # set a format which is simpler for console use
        formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')
        # tell the handler to use this format
        console.setFormatter(formatter)
        # add the handler to the root logger
        logging.getLogger().addHandler(console)
        
        # Now, we can log to the root logger, or any other logger
        logging.info('Practical Data Science is fun!.')
        
        for sLevel in sLevels:
            sApp='Application-'+ sCompany + '-' + sLayer + '-' + sLevel
            logger = logging.getLogger(sApp)
            if sLevel == 'debug':
                logger.debug('Practical Data Science logged a debugging message.')
            if sLevel == 'info':
                logger.info('Practical Data Science logged information message.')
            if sLevel == 'warning':
                logger.warning('Practical Data Science logged a warning message.')
            if sLevel == 'error':
                logger.error('Practical Data Science logged an error message.')

        `
        const text13 = `
        (R)
library(readr) 
IP_DATA_ALL<- read_csv("C:/Users/Dell/OneDrive/Desktop/DSPractical/IP_DATA_ALL.csv") View(IP_DATA_ALL) 
spec(IP_DATA_ALL) 
library(tibble) 
set_tidy_names(IP_DATA_ALL,syntactic = TRUE,quiet = FALSE) IP_DATA_ALL_FIX=set_tidy_names(IP_DATA_ALL,syntacti c=TRUE,quiet=FALSE) sapply(IP_DATA_ALL_FIX,typeof) 
library(data.table) hist_country=data.table(Country=unique(IP_DATA_ALL_FIX[i s.na(IP_DATA_ALL_FIX['Country'])==0,]$Country))
setorder(hist_country,'Country') hist_country_with_id=rowid_to_column(hist_country,var="Row IDCountry") 
IP_DATA_COUNTRY_FREQ=data.table(with(IP_DATA_AL L_FIX,table(Country))) 
View(IP_DATA_COUNTRY_FREQ) 
sapply(IP_DATA_ALL_FIX[,'Latitude'],min,na.rm=TRUE) 
sapply(IP_DATA_ALL_FIX[,'Country'],min,na.rm=TRUE) 
sapply(IP_DATA_ALL_FIX[,'Latitude'],max,na.rm=TRUE) 
sapply(IP_DATA_ALL_FIX[,'Country'],max,na.rm=TRUE) 
sapply(IP_DATA_ALL_FIX[,'Latitude'],mean,na.rm=TRUE) 
sapply(IP_DATA_ALL_FIX[,'Latitude'],median,na.rm=TRUE) 
sapply(IP_DATA_ALL_FIX[,'Latitude'],quantile,na.rm=TRUE 
sapply(IP_DATA_ALL_FIX[,'Latitude'],range,na.rm=TRUE) 
sapply(IP_DATA_ALL_FIX[,'Latitude'],sd,na.rm=TRUE) 
sapply(IP_DATA_ALL_FIX[,'Longitude'],sd,na.rm=TRUE)
        `
        const text14 = `
        import sys 
import os import pandas as pd 
sFileName="C:/Users/Dell/OneDrive/Desktop/DS Practical/IP_DATA_ALL.csv" IP_DATA_ALL=pd.read_csv(sFileName,header=0,low_memor y=False,encoding="latin-1") 
sFileDir="C:/Users/Dell/OneDrive/Desktop/DS Practical" if not os.path.exists(sFileDir):
os.makedirs(sFileDir) 
print('Rows:',IP_DATA_ALL.shape[0]) 
print('Columns:',IP_DATA_ALL.shape[1]) 
print('Raw Data') 
for i in range(0,len(IP_DATA_ALL.columns)):
print(IP_DATA_ALL.columns[i],
type(IP_DATA_ALL.columns [i])) 
print('Fixed Data') IP_DATA_ALL_FIX=IP_DATA_ALL 
for i in range(0,len(IP_DATA_ALL.columns)):
cNameOld=IP_DATA_ALL_FIX.columns[i]+' ' cNameNew=cNameOld.strip().replace(" ",".") IP_DATA_ALL_FIX.columns.values[i]=cNameNew
print(IP_DATA_ALL.columns[i],type(IP_DATA_ALL.columns [i]))
print(IP_DATA_ALL_FIX.head()) print('Fixed Data Set with ID') 
IP_DATA_ALL_with_ID=IP_DATA_ALL_FIX 
IP_DATA_ALL_with_ID.index.names=['RowID'] 
sFileName2='C:/Users/Dell/OneDrive/Desktop/DS Practical/Retrieve_IP_DATA.csv' 
IP_DATA_ALL_with_ID.to_csv(sFileName2,index=True,enco ding="latin-1")
        `
        const text15 = `
        (R)
library(readr) 
library(data.table) 
FileName <- "C:/Users/Dell/OneDrive/Desktop/DS Practical/IP_DATA_ALL_1.csv" IP_DATA_ALL <- read_csv(FileName) 
hist_country <- data.table(Country = unique(IP_DATA_ALL$Country)) 
pattern_country <- data.table(Country = hist_country$Country, PatternCountry = hist_country$Country) 
oldchar <- c(letters, LETTERS, "0", "1", "2", "3", "4", "5", "6", "7", "8", "9", " ", "~", "!", "@", "#", "$", "%", "^", "&", "*", "(", ")", "-", "_", "=", "+", "{", "}", "[", "]", "|", "\\", ":", ";", "'", "\"", "<", ",", ".", ">", "/", "?") 
newchar <- rep("A", length(letters) + length(LETTERS)) # For letters 
newchar <- c(newchar, rep("N", 10)) # For digits
newchar <- c(newchar, "b", rep("u", length(oldchar) - length(letters) - length(LETTERS) - 10 - 1)) # For space and special characters
translation_table <- setNames(newchar, oldchar) 
replace_chars <- function(s, translation_table) { 
for (old in names(translation_table)) { 
s <- chartr(old, translation_table[old], s) 
}
return(s) 
} 
pattern_country[, PatternCountry := sapply(PatternCountry, replace_chars, translation_table)] 
View(pattern_country)

        `
        const text16 = `
        import sys 
import os 
import pandas as pd 
sFileName='C:/Users/Dell/OneDrive/Desktop/DS Practical/IP_DATA_ALL.csv' IP_DATA_ALL=pd.read_csv(sFileName) 
sFileDir='C:/Users/Dell/OneDrive/Desktop/DS Practical' 
if not os.path.exists(sFileDir): 
os.makedirs(sFileDir) 
print('Rows:', IP_DATA_ALL.shape[0]) 
print('Columns:', IP_DATA_ALL.shape[1]) 
print(' Raw Data Set ') 
for i in range(0,len(IP_DATA_ALL.columns)):
print(IP_DATA_ALL.columns[i],
type(IP_DATA_ALL.columns [i])) print(' Fixed Data Set ') IP_DATA_ALL_FIX=IP_DATA_ALL 
for i in range(0,len(IP_DATA_ALL.columns)): 
cNameOld=IP_DATA_ALL_FIX.columns[i] + ' '
cNameNew=cNameOld.strip().replace(' ', '.') 
IP_DATA_ALL_FIX.columns.values[i] = cNameNew print(IP_DATA_ALL.columns[i],
type(IP_DATA_ALL.columns [i])) 
print('Fixed Data Set with ID')
IP_DATA_ALL_with_ID=IP_DATA_ALL_FIX 
IP_DATA_ALL_with_ID.index.names = ['RowID'] print(' Done!! ')

        `
        const text17 = `
        import pandas as pd 
from sklearn.preprocessing 
import StandardScaler 
url = 'https://archive.ics.uci.edu/ml/machine-learningdatabases/iris/iris.data' 
df = pd.read_csv(url, header=None, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']) 
print("First few rows of the dataset:") 
print(df.head()) 
print("\nDataFrame info:") 
print(df.info()) 
print("\nSummary statistics:") 
print(df.describe()) 
missing_values = df.isnull().sum() 
if missing_values.any():
print("\nMissing values in the dataset:")
print(missing_values[missing_values > 0]) 
else: 
print("\nNo missing values in the dataset.") 
duplicates = df.duplicated().sum() 
print('Total duplicate rows: {duplicates}') 
df_unique = df.drop_duplicates() 
print('Unique rows after removing duplicates: {len(df_unique)}') 
Q1 = df['sepal_length'].quantile(0.25) 
Q3 = df['sepal_length'].quantile(0.75) 
IQR = Q3 - Q1 
outliers = df[(df['sepal_length'] < (Q1 - 1.5 * IQR)) | (df['sepal_length'] > (Q3 + 1.5 * IQR))] print('\nOutliers in sepal_length:\n{outliers}') 
scaler = StandardScaler() 
df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']] = scaler.fit_transform(df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]) 
df = pd.get_dummies(df, columns=['species'], drop_first=True) 
print("\nFinal check for missing values:") 
print(df.isnull().sum()) 
print('\nFinal count of duplicate rows: {df.duplicated().sum()}')

        `
       
        

        // Add the rest of your text blocks here (text6, text7, ...)

        // Function to copy text using Clipboard API
        async function copyText(text) {
            try {
                // Use Clipboard API to write text to the clipboard
                await navigator.clipboard.writeText(text);

                // Notify user of success
                const messageElement = document.getElementById("copiedMsg");
                messageElement.innerText = "Text copied successfully!";
                messageElement.style.color = "green";

                // Clear message after 2 seconds
                setTimeout(() => {
                    messageElement.innerText = "";
                }, 2000);
            } catch (err) {
                // Notify user of failure
                const messageElement = document.getElementById("copiedMsg");
                messageElement.innerText = "Failed to copy text. Check your browser permissions.";
                messageElement.style.color = "red";

                console.error("Error copying text: ", err);
            }
        }
    </script>

</body>

</html>
